{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time, random, keras, pickle, gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from face_verification.facenet import basenet\n",
    "from face_verification.facenet import triplet_net\n",
    "from face_verification.facenet import triplet_loss\n",
    "from face_verification.facenet import train_triplet_generator\n",
    "from face_verification.facenet import test_triplet_generator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg2_train = pd.read_csv('./images/vgg2_train_path.csv')\n",
    "vgg2_test = pd.read_csv('./images/vgg2_test_path.csv')\n",
    "\n",
    "print('Train:\\t# person ={0:5d}\\t # images ={1:8d}'.format(len(vgg2_train['name'].unique()), \n",
    "                                                           len(vgg2_train)))\n",
    "print('Test:\\t# person ={0:5d}\\t # images ={1:8d}'.format(len(vgg2_test['name'].unique()),\n",
    "                                                          len(vgg2_test)))\n",
    "\n",
    "vgg2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the train and test dataframe\n",
    "def path_to_list(df):\n",
    "    \"\"\" function to merge df into the name and path list format \"\"\"\n",
    "    paths = list(df['path'].values)\n",
    "    count = len(paths)\n",
    "    \n",
    "    return pd.Series([count, paths], index=['count', 'paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg2_train_df = vgg2_train.groupby('name').apply(path_to_list).reset_index()\n",
    "vgg2_test_df = vgg2_test.groupby('name').apply(path_to_list).reset_index()\n",
    "\n",
    "vgg2_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Triplet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproduciable purpose\n",
    "seed = 42\n",
    "K.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(seed)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the base-cnn model\n",
    "base_model = basenet(output_shape=128)\n",
    "\n",
    "# # visualization\n",
    "# plot_model(base_model, show_shapes=True, to_file='./results/base-model.png')\n",
    "# plot_model(base_model, show_shapes=True, to_file='./results/base-model.pdf')\n",
    "\n",
    "# base-model summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the triplet-network model\n",
    "triplet_model = triplet_net(base_model=base_model, input_shape=(96, 96, 3))\n",
    "\n",
    "# # visualization\n",
    "# plot_model(triplet_model, show_shapes=True, to_file='./results/triplet-model.png')\n",
    "# plot_model(triplet_model, show_shapes=True, to_file='./results/triplet-model.pdf')\n",
    "\n",
    "# base-model summary\n",
    "triplet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning scheduler\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\" Learning rate schedule \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 400:\n",
    "        lr *= 2e-1\n",
    "    elif epoch > 300:\n",
    "        lr *= 4e-1\n",
    "    elif epoch > 200:\n",
    "        lr *= 6e-1\n",
    "    elif epoch > 100:\n",
    "        lr *= 8e-1\n",
    "        \n",
    "    return lr\n",
    "\n",
    "# define optimizer\n",
    "opt = keras.optimizers.Adam(lr=lr_schedule(0))\n",
    "\n",
    "# create checkpoint folder\n",
    "path = './models/margin-05-' + time.strftime('%Y%m%d-%H%M')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "print('Checkpoing Folder:\\t', path)    \n",
    "\n",
    "# create call backs\n",
    "checkpoint = ModelCheckpoint(filepath=path + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                             monitor='val_loss', verbose=0, save_best_only=False, \n",
    "                             save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# compile the model\n",
    "triplet_model.compile(optimizer=opt, loss=triplet_loss(0.5))\n",
    "\n",
    "# define training and test dataset image generator\n",
    "train_generator = train_triplet_generator(vgg2_train_df, batch_size=128)\n",
    "test_generator = test_triplet_generator(vgg2_test_df, batch_size=100, loops=4, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = triplet_model.fit_generator(train_generator, steps_per_epoch=67, epochs=500, \n",
    "                                      validation_data=test_generator, validation_steps=20, \n",
    "                                      verbose=2, callbacks=callbacks)\n",
    "\n",
    "base_model.save(path + '/facenet-model.h5')\n",
    "pickle.dump(history.history, open(path + '/facenet-history.p', 'wb'))\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training process\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(train_loss, label='Training Loss')\n",
    "ax.plot(val_loss, label='Validation Loss')\n",
    "ax.set_title('Loss vs. Epochs', fontsize=16)\n",
    "ax.set_xlabel('Epochs', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.legend(fontsize=14)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
